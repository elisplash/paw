---
title: "Ollama"
description: "That's it — no API key or configuration needed."
---

# Ollama

[Ollama](https://ollama.com/) runs open-source AI models locally on your machine. Free, private, no API key needed.

## Setup

1. Install Ollama: [ollama.com/download](https://ollama.com/download)
2. Pull a model:
   ```bash
   ollama pull llama3.1
   ```
3. Pawz auto-detects Ollama at `localhost:11434`

That's it — no API key or configuration needed.

## Configuration

| Setting | Default | Notes |
|---------|---------|-------|
| Base URL | `http://localhost:11434/v1` | Change if Ollama runs on another machine |
| API key | (none) | Not needed for local Ollama |
| Default model | — | Set to your preferred model |

## Recommended models

| Model | Size | Best for |
|-------|------|----------|
| `llama3.1:8b` | 4.7 GB | General use, fast |
| `llama3.1:70b` | 40 GB | High quality, needs GPU |
| `gemma2:9b` | 5.4 GB | Good balance |
| `mistral:7b` | 4.1 GB | Fast, European |
| `codellama:13b` | 7.4 GB | Code generation |
| `deepseek-coder-v2` | 8.9 GB | Code generation |

## Embedding model

Pawz uses `nomic-embed-text` for the memory system:

```bash
ollama pull nomic-embed-text
```

Pawz auto-pulls this model if it's missing.

## Remote Ollama

To use Ollama on another machine:

1. On the remote machine, set `OLLAMA_HOST=0.0.0.0:11434`
2. In Pawz, change the base URL to `http://<remote-ip>:11434/v1`

## Tips

- Ollama uses your GPU automatically if available
- Models are downloaded to `~/.ollama/models/`
- Run `ollama list` to see installed models
- Run `ollama rm <model>` to delete a model
- Ollama models use the `:` syntax (e.g., `llama3.1:8b`) — Pawz auto-routes these
