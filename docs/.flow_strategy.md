# Flow Strategy — OpenPawz

> Internal roadmap for the Flows visual pipeline feature.
> Introducing **The Conductor Protocol** — the third pillar of the OpenPawz
> invention trilogy alongside The Librarian Method and The Foreman Protocol.


---

## The OpenPawz Trinity

| Protocol | Question Answered | Innovation |
|----------|-------------------|------------|
| **Librarian Method** | *Which* tools exist for this intent? | Intent-driven discovery via embeddings |
| **Foreman Protocol** | *How* to execute actions cheaply? | Worker model delegation via self-describing MCP |
| **Conductor Protocol** | *What's the optimal way to execute this blueprint?* | AI-compiled flow execution with collapse, extract, parallelize, converge |

Together they compose: the Librarian discovers tools, the Conductor compiles the
execution strategy, and the Foreman executes deterministic actions — enabling
AI-orchestrated workflows over 25,000+ integrations at speeds and costs no
existing platform can match.

---

## The Conductor Protocol

**Intent-Compiled Flow Execution via Blueprint Interpretation**

*The third OpenPawz invention. Open source (MIT).*

### The Problem

Every workflow platform — Zapier, n8n, Make, Temporal, Airflow, Prefect — treats
a flow graph as source code: a sequential program walked node by node. Each node
is a discrete function call. Data pipes through edges as strings. This model was
designed for deterministic API-to-API automation in the pre-AI era.

Pawz nodes aren't API calls — they're AI agents. An agent told to "Research this
topic, draft an email, and review the draft" can do all three in a single turn.
Splitting that into three separate LLM calls with string-piping between them:

1. **Wastes time** — 3 LLM round-trips instead of 1
2. **Destroys context** — each node starts cold, re-reads upstream as plain text
3. **Loses the thread** — no node knows the broader goal of the flow

It's like hiring a brilliant writer and saying: "Step 1: Research. Stop. Hand me
your notes. Step 2: Here are some notes someone wrote, draft an email. Stop."
Each step loses continuity.

### The Invention

The Conductor Protocol treats the flow graph as a **blueprint of intent**, not a
program to execute literally. Before execution, a Conductor reads the entire
blueprint and **compiles it into an optimized execution strategy**.

```
Traditional:  Graph → Sequential Walker → Node → Node → Node → Node → Node
Conductor:    Graph → AI Compiler → Optimized Strategy → Batched execution
```

The Conductor performs four transformations:

#### 1. Collapse

Adjacent agent nodes that form a coherent task merge into a single LLM call.

```
Before: [Research] → [Draft email] → [Review draft]
After:  [Research, draft, and review in one turn]

3 LLM round-trips → 1. Same quality. Full context preserved.
```

Rules: same agent profile, same model, no condition/branch between them, no
side-effecting action nodes in the chain. Conservative by default — even
collapsing 3 nodes into 1 is a 3x speedup on that segment.

#### 2. Extract

Deterministic actions are separated from creative/reasoning work and routed
directly to the Foreman for zero-LLM execution.

```
Before: [Agent: "send email to john@..."] → engineChatSend → LLM thinks → MCP call
After:  [Direct MCP call via Foreman] → no LLM involved

5 seconds → 200ms. The AI composed the content upstream; the action just sends it.
```

Node kind determines routing: `tool`/`code`/`output`/`http` → direct execution.
`agent`/`data` → LLM call. `condition` → structured eval first, AI fallback
only for semantic conditions.

#### 3. Parallelize

Independent branches are recognized as concurrent work and executed
simultaneously.

```
Before: [Fetch GitHub PRs] → wait → [Check Slack] → wait → [Query DB]
After:  [Fetch GitHub PRs ‖ Check Slack ‖ Query DB] → all at once

15 seconds → 5 seconds. Three independent actions, one wall-clock period.
```

Detection is deterministic graph analysis: nodes at the same depth with no shared
edges and no data dependency → `Promise.all`.

#### 4. Converge (Convergent Mesh)

Cyclic subgraphs — impossible for every other workflow platform — are handled as
iterative shared-context spaces where agents converse until outputs stabilize.

```
Before: Agent A ↔ Agent B → BREAKS (topological sort fails on cycles)
After:  Convergent Mesh:
  Round 1: A states position, B responds
  Round 2: A counters, B counters
  Round 3: Outputs stabilizing → convergence → stop
  → Synthesizer reads full history → final output
```

This is something n8n *literally cannot do*. Its execution model requires
directed acyclic graphs. Cycles are errors. In Pawz, cycles are conversations.

Max iterations: 5 (configurable). Convergence detection: compare consecutive
outputs. Forced synthesis on timeout.

---

## Compiled Strategy Example

**Flow: "Customer Complaint Handler" (5 nodes)**

```
┌──────────────────────────────────────────────────────────────────┐
│ CONDUCTOR COMPILED STRATEGY                                      │
│                                                                  │
│ Phase 1 (parallel):                                              │
│   ├─ Unit A [collapsed-agent]: Nodes 1,2,3                      │
│   │   "Read the complaint, research customer history,            │
│   │    draft a response"                                         │
│   │   → 1 LLM call covering 3 flow nodes                        │
│   │                                                              │
│   └─ Unit B [direct-action]: Node 4                              │
│       "Log complaint to Jira"                                    │
│       → Direct MCP via Foreman, no LLM                           │
│                                                                  │
│ Phase 2 (sequential, depends on Phase 1):                        │
│   └─ Unit C [direct-action]: Node 5                              │
│       "Send response email"                                      │
│       → Direct MCP with Unit A's output                          │
└──────────────────────────────────────────────────────────────────┘
```

---

## Speed Comparison: Pawz Conductor vs n8n vs Current Pawz

### Methodology

Estimated timings based on measured component latencies:
- LLM API call (cloud, streaming): 2–8s depending on complexity
- LLM API call (local Ollama 7B): 1–4s
- MCP tool call via Foreman (direct): 100–500ms
- Tauri IPC round-trip: ~5ms
- HTTP API call (external service): 100–800ms
- Conductor compilation (single LLM call to read blueprint): 1–3s
- Polling overhead (current executor per node): 250ms–1s
- Session setup/teardown overhead (current executor per node): ~200ms

### Scenario 1: Simple Linear Flow (5 nodes)

**Trigger → Agent (research) → Agent (draft) → Tool (send email) → Output**

| Platform | Calls | Wall Clock | Why |
|----------|-------|-----------|-----|
| **n8n** | 4 API calls | ~2s | Direct API calls, no AI, deterministic |
| **Pawz Current** | 5 LLM calls (sequential) | **20–45s** | Each node = LLM round-trip + polling + session overhead |
| **Pawz + Conductor** | 1 LLM call + 1 MCP call | **4–9s** | Nodes 2+3 collapsed, node 4 extracted, output passthrough |

**Speedup: 4–5x faster than current Pawz. Comparable to n8n for the AI work,
plus AI capabilities n8n can't do.**

### Scenario 2: Parallel Fan-Out (8 nodes)

**Trigger → [GitHub PRs ‖ Slack msgs ‖ Jira tickets] → Agent (summarize) → Send report → Output**

| Platform | Calls | Wall Clock | Why |
|----------|-------|-----------|-----|
| **n8n** | 5 API calls | ~3s | Parallel API calls + sequential merge/send |
| **Pawz Current** | 8 LLM calls (sequential) | **35–70s** | Every node waits for the previous, even independent branches |
| **Pawz + Conductor** | 3 MCP calls (parallel) + 1 LLM call + 1 MCP call | **5–10s** | 3 fetches extracted + parallelized, summarize = 1 LLM, send = direct MCP |

**Speedup: 5–7x faster than current Pawz.**

### Scenario 3: Complex Bidirectional Debate (6 nodes)

**Trigger → Agent A ↔ Agent B (3 rounds) → Synthesizer → Output**

| Platform | Calls | Wall Clock | Why |
|----------|-------|-----------|-----|
| **n8n** | ❌ Impossible | N/A | Cannot execute cycles. Graph validation error. |
| **Pawz Current** | ❌ Broken | N/A | Topological sort fails, appends nodes randomly |
| **Pawz + Conductor** | 1 mesh (3 rounds × 2 agents) + 1 LLM call | **15–25s** | Convergent Mesh handles cycles, synthesizer collapses to 1 call |

**Speedup: ∞ — this flow type doesn't work in current Pawz or n8n at all.**

### Scenario 4: Large Production Flow (20 nodes)

**Webhook → 4 parallel data fetches → condition branch → 3 agent chains (×2 branches) → 4 actions → error handler → output**

| Platform | Calls | Wall Clock | Why |
|----------|-------|-----------|-----|
| **n8n** | 16 API calls | ~8s | Parallel where possible, all deterministic |
| **Pawz Current** | 20 LLM calls (sequential) | **80–160s** | Every node sequential, every action through LLM |
| **Pawz + Conductor** | Compile (1) + 4 MCP parallel + 2 LLM (collapsed chains) + 4 MCP parallel + 1 passthrough | **8–18s** | 6 agent nodes → 2 collapsed LLM calls; 8 action nodes → direct MCP; 4 fetches parallelized |

**Speedup: 8–10x faster than current Pawz. Within 2x of pure n8n, plus AI.**

### Summary

| Scenario | Current Pawz | Pawz + Conductor | n8n | Conductor vs Current |
|----------|-------------|------------------|-----|---------------------|
| 5-node linear | 20–45s | 4–9s | ~2s | **4–5x faster** |
| 8-node fan-out | 35–70s | 5–10s | ~3s | **5–7x faster** |
| 6-node bidirectional | Broken | 15–25s | Impossible | **∞ (enables new capability)** |
| 20-node production | 80–160s | 8–18s | ~8s | **8–10x faster** |

The pattern: the larger and more complex the flow, the bigger the Conductor's
advantage — because there are more opportunities to collapse, extract, and
parallelize. Simple 2-node flows get minimal benefit (and skip the Conductor
entirely via the adaptive threshold).

### Why Pawz Can't Match Raw n8n Speed (and Doesn't Need To)

n8n nodes are direct API calls — 100-500ms each. No AI reasoning involved.
Pawz agent nodes involve LLM inference — 2-8s each. That's the inherent cost
of intelligence. The Conductor minimizes the *number* of LLM calls, but each
call still takes seconds.

The value proposition isn't "faster than n8n for deterministic tasks." It's:
**"As fast as possible for tasks that require AI reasoning, with deterministic
tasks executing at n8n speed via the Foreman, and capabilities (cycles,
collapse, intent compilation) that n8n fundamentally cannot offer."**

---

## Current Codebase State

- ~5,500 lines TypeScript + 1,690 lines CSS
- Pure SVG canvas (no library dependencies)
- 9 node kinds, 4 edge types, 22 templates
- NLP parser (arrow, numbered, pipe, prose syntax)
- Cron scheduling (browser-side `setInterval`)
- Debug mode with breakpoints + step-through
- Error-path routing + exponential backoff retries
- AI agent integration with streaming
- localStorage persistence only
- Strictly sequential execution (no parallelism)
- Full SVG re-render on every interaction

---

## Performance Bottleneck Analysis (Current Executor)

### P0 — Every node = full LLM round-trip

Agent/tool/data/condition nodes all call `engineChatSend()` → Tauri IPC → Rust →
LLM provider → stream back. Even deterministic actions like "send email" route
through an AI agent. A 5-node flow = 5 sequential LLM calls, each taking 2–10s.

**Conductor fix:** Extract + Collapse. Deterministic actions bypass LLM via
Foreman. Adjacent agent nodes merge into single LLM calls.

### P1 — Sequential execution

`for (let i = 0; i < plan.length; i++)` walks nodes one-by-one. Independent
branches that could run in parallel instead wait sequentially.

**Conductor fix:** Parallelize. Independent branches detected via graph analysis
and executed concurrently.

### P2 — Polling-based stream completion

After `engineChatSend()`, the executor polls every 250ms to check if the stream
finished, plus a 1-second fallback timeout for sync responses. Adds 250ms–1s of
artificial latency per node.

**Fix:** Replace polling with event-driven completion. Resolve the promise from
the `subscribeSession` callback when the stream ends.

### P3 — Full SVG nuke-and-rebuild

`renderGraph()` clears all SVG groups and rebuilds every element from scratch.
Called on every drag frame. For 20 nodes + 25 edges = ~200 SVG elements
destroyed/created per mouse move event.

**Fix:** `requestAnimationFrame` throttle during drag. Dirty-region rendering.
Node ID → SVG element map for O(1) updates.

### P4 — O(n) linear scans

Node lookups use `graph.nodes.find()` everywhere. During drag at 60fps with
multiple finds per render = thousands of linear scans per second.

**Fix:** `Map<string, FlowNode>` index. O(1) lookups.

### P5 — Bidirectional flows break DAG execution

Kahn's topological sort assumes DAG. Bidirectional edges create cycles. Code
appends remaining nodes in arbitrary order.

**Conductor fix:** Convergent Mesh. Cyclic subgraphs iterate until outputs
stabilize.

---

## Implementation Phases

### Phase 0 — Quick Wins (No Conductor Required)

> **Goal:** Immediate speed improvements via engineering fixes.
> **Effort:** Small. **Impact:** Noticeable.

#### 0.1 Event-Driven Stream Completion

- Add `onComplete` callback to `subscribeSession`
- Resolve executor promise from callback instead of 250ms polling
- Remove the 1-second fallback setTimeout
- Saves 250ms–1s per node (free speed)

#### 0.2 Canvas Performance

- `requestAnimationFrame` throttle for `renderGraph()` during drag
- Build `Map<string, FlowNode>` and `Map<string, FlowEdge>` indexes
- Dirty-region rendering: track changed node IDs, only rebuild those SVG elements
- Viewport culling: don't render nodes outside visible area

#### 0.3 Node Index Maps

- Replace all `graph.nodes.find(n => n.id === id)` with `nodeMap.get(id)`
- Replace all `graph.edges.filter(e => e.from === id)` with adjacency list
- Build maps on graph load, update incrementally on add/remove

---

### Phase 1 — Foundation

> **Goal:** Stability, persistence, and minimum expected features.
> **Effort:** Medium. **Impact:** High.

#### 1.1 Rust Backend Persistence

- Add Tauri commands: `engine_flows_save`, `engine_flows_load`,
  `engine_flows_list`, `engine_flows_delete`
- Store flows in SQLite alongside agents/sessions
- Migrate from localStorage on first launch
- Auto-save on graph change (debounced 1s)
- Keep localStorage as offline fallback

#### 1.2 Backend Scheduling

- Move cron tick to Rust side-thread
- Flows run when app is open, not just when tab is focused
- Persistent schedule registry in database
- Fire log with run history

#### 1.3 Undo/Redo

- Command stack pattern: push graph snapshots on every mutation
- Ctrl+Z / Ctrl+Shift+Z keybindings
- Cap stack at ~50 entries
- Visual undo/redo buttons in toolbar

#### 1.4 Import/Export

- Export: toolbar button → `serializeGraph()` → download as `.pawflow.json`
- Import: toolbar button → file picker → `deserializeGraph()` → add to list
- Validate schema on import with user-friendly errors

#### 1.5 Execution History

- Persist `FlowExecEvent[]` per run in database
- Add "Runs" tab in sidebar with status, duration, timestamp
- Click a run to see per-node results
- Re-run button on each history entry

---

### Phase 2 — The Conductor Protocol

> **Goal:** Implement AI-compiled flow execution.
> **Effort:** Large. **Impact:** Transformative.

Each step ships independently. Each step makes flows faster. The full Conductor
is the sum of these parts.

#### 2.1 Extract (Deterministic Action Bypass)

The simplest Conductor primitive. No AI compilation needed — pure static
analysis.

- New `http` node kind: method, URL, headers, body, response mapping
- New `mcp-tool` node kind: direct MCP tool invocation without agent proxy
- These nodes bypass `engineChatSend()` entirely — direct Tauri IPC to Rust
  which makes the HTTP/MCP call and returns structured data
- Route existing `tool` nodes through Foreman when prompt is empty (action-only)
- Expected improvement: 2–8s → 200ms per deterministic node

#### 2.2 Parallelize (Concurrent Branch Execution)

Deterministic graph analysis — no AI needed.

- In `buildExecutionPlan()`, group nodes by depth level
- Nodes at the same depth with no mutual data dependencies → `Promise.all`
- Report parallel progress via separate event streams
- Visual indicator: parallel nodes animate simultaneously
- Expected improvement: N sequential branches → 1 wall-clock period

#### 2.3 Collapse (Agent Node Merging)

The first AI-assisted primitive. Conservative heuristic, no Conductor agent
needed yet.

- Detect chains of consecutive agent nodes with:
  - Same agent profile or no agent specified
  - Same model or no model override
  - No condition/branch nodes between them
  - No side-effecting action nodes in the chain
- Merge their prompts into a single compound prompt:
  `"Step 1: {node1.prompt}\nStep 2: {node2.prompt}\nStep 3: {node3.prompt}"`
- Execute as one LLM call, parse structured output back to individual node states
- Configurable: user can mark nodes as "do not collapse" in config
- Expected improvement: N adjacent agent nodes → 1 LLM call

#### 2.4 Converge (Convergent Mesh for Cycles)

The novel primitive. Enables flow types impossible in any other platform.

- Detect cycles in graph via DFS during execution planning
- For acyclic subgraphs: standard topological execution
- For cyclic subgraphs: create a Convergent Mesh
  - Shared context space visible to all agents in the cycle
  - Iterative rounds: each agent sees all previous outputs
  - Convergence detection: cosine similarity between consecutive round outputs
    (using the same embedding infra as the Librarian)
  - Hard cap: 5 iterations (configurable per mesh)
  - On convergence or cap: forced synthesis step
- Visual: mesh nodes rendered in a shared boundary box with round counter

#### 2.5 Full Conductor Agent

The capstone. An AI agent reads the entire flow blueprint and produces the
compiled strategy using the four primitives.

- Conductor runs as a single LLM call before execution begins
- Input: serialized FlowGraph JSON + available agent profiles + node configs
- Output: ExecutionStrategy JSON (phases, units, collapse groups, mesh configs)
- Strategy validation: every node appears exactly once, dependencies respected,
  deterministic nodes correctly classified
- If validation fails: fall back to sequential executor (safe path)
- Adaptive threshold: only invoke Conductor for flows with 4+ nodes, or any
  branching, or any cycles. Below that, run sequential executor directly.
- Conductor overhead: 1–3s (amortized across the 5–50s saved)

**Conductor prompt specification:**

```
You are the Conductor for an AI workflow execution engine. You will receive a
flow graph (JSON) describing a pipeline of AI agent nodes, tool nodes, code
nodes, and action nodes.

Your job is to compile an optimal execution strategy using these primitives:
- COLLAPSE: Merge adjacent agent nodes into a single LLM call
- EXTRACT: Route deterministic nodes (tool, code, http) to direct execution
- PARALLELIZE: Run independent branches concurrently
- CONVERGE: Handle cyclic subgraphs as iterative meshes

Output a JSON ExecutionStrategy with phases, units, and configurations.
Do not execute anything. Only produce the plan.
```

---

### Phase 3 — Power Features

> **Goal:** Feature parity with workflow automation platforms, enhanced by
> the Conductor's execution model.

#### 3.1 Loop / Iteration Node

- New `loop` node kind: forEach over array data
- Config: input array expression, iteration variable name, max iterations
- Conductor integration: loops are extracted as iteration units, not expanded
  into N copies of the subgraph
- Visual: iteration counter badge during execution

#### 3.2 Variables & Global State

- Flow-level key-value store: `{{flow.varName}}` syntax
- Set Variable node or inline config on any node
- Environment variables: `{{env.KEY}}`
- Previous run results: `{{lastRun.nodeLabel.output}}`
- Conductor integration: variables inform collapse decisions (shared state
  between nodes prevents collapse)

#### 3.3 Sub-flows

- Implement the `group` node kind (currently dead code)
- Config: select an existing flow to embed
- Conductor integration: sub-flows get their own compiled strategy (recursive)
- Visual: collapsible group box with preview of inner nodes
- Max recursion depth: 5

#### 3.4 Credential Vault

- Encrypted credential storage in Rust backend
- Credential types: API Key, OAuth2, Basic Auth, Bearer Token
- Reference credentials in HTTP/MCP/API nodes by name
- Credential test button (validate before save)

#### 3.5 Multi-Select & Clipboard

- Box select (Shift + drag on empty space)
- Shift+click to add to selection
- Ctrl+C/V for copy/paste selected nodes
- Bulk delete, bulk move, duplicate with offset

#### 3.6 Edge Editing

- Click to select edges (visual highlight)
- Delete selected edge (Delete key or context menu)
- Edit edge kind (forward/reverse/bidirectional/error) via popup
- Add/edit edge labels and conditions

---

### Phase 4 — AI Superpowers (Unique to Pawz)

> **Goal:** Capabilities that n8n/Zapier fundamentally cannot replicate.
> These build on the Conductor Protocol's foundation.

#### 4.1 AI Flow Builder

- Upgrade NLP parser to use the actual AI engine
- User describes intent in chat → agent builds complete `FlowGraph`
- Agent understands available agents, tools, integrations
- Iterative refinement: "add error handling" → agent modifies the graph
- "Explain this flow" → agent walks graph and produces description
- Agent can preview the Conductor's compiled strategy before execution

#### 4.2 Agent Self-Healing

- On node failure: offer "Ask agent to diagnose"
- Agent inspects error, node config, upstream data
- Proposes config changes or prompt adjustments
- User approves → agent modifies node → auto-retry
- Conductor re-compiles strategy around the fix
- Learns from fixes via memory integration (Librarian)

#### 4.3 Squad Node

- New `squad` node kind: invokes a Squad (multi-agent team) as a flow step
- Config: select squad, define objective, set timeout
- Conductor integration: squad nodes are never collapsed (they're already
  multi-agent orchestration internally)
- Visual: squad member avatars inside the node

#### 4.4 Flow Suggestions / Autocomplete

- After placing a node, suggest likely next nodes based on:
  - Template patterns
  - Common sequences in user's flow history
  - Agent capability matching
- Ghost node preview: semi-transparent suggested node with one-click add
- Conductor preview: "if you add this node, the strategy would collapse
  these 3 nodes into 1 call"

#### 4.5 Smart Condition Evaluation

- Structured expression evaluator for data conditions:
  `input.status === 200`, `output.length > 0`, `data.items.count >= 10`
- JSONPath / jq-style data access
- Falls back to AI evaluation only for fuzzy/semantic conditions
- Conductor integration: structured conditions are extracted (no LLM needed),
  AI conditions remain as agent calls

#### 4.6 Memory-Aware Flows

- Nodes can read/write to agent memory (Librarian system)
- Flow context persists across scheduled runs
- "Remember the last 5 results" pattern for trend analysis
- Cross-flow memory sharing for related pipelines
- Conductor integration: memory reads are extracted (fast local lookup),
  memory writes are deferred to end of phase

---

### Phase 5 — Sick UI

> **Goal:** Visual polish that makes the canvas feel alive and professional.

#### 5.1 Minimap

- Thumbnail overview in bottom-right corner
- All nodes as colored dots, viewport rectangle (draggable)
- Toggle visibility with toolbar button

#### 5.2 Node Data Previews

- After execution: last output as a small badge on the node
- Truncated ~40 chars, expand-on-hover
- Color-coded by data type (string=blue, JSON=purple, error=red)

#### 5.3 Edge Data Labels

- Data shape on edges: "string", "JSON[]", "number"
- Inferred from upstream node output, toggle in toolbar

#### 5.4 Animated Execution Flow

- During execution: particle/pulse animation along edges
- Conductor phases animate as groups — parallel branches pulse simultaneously
- CSS `offset-path` along bezier edge paths

#### 5.5 Color-Coded Node Headers

- Each node kind gets a distinct accent color on its header stripe:
  - Trigger: kinetic-gold | Agent: accent (blue) | Tool: kinetic-sage
  - Condition: orange | Data: cyan | Code: violet
  - Output: kinetic-steel | Error: kinetic-red
  - HTTP: teal | Loop: green | Squad: purple

#### 5.6 Connection Validation

- While dragging: valid targets glow green, invalid show red
- Snap-to-port within 20px

#### 5.7 Alignment Guides

- Smart guides when dragging (Figma-style)
- Horizontal/vertical alignment lines, equal spacing indicators
- Snap-to-alignment with 5px threshold

#### 5.8 Conductor Strategy Visualization

- Before execution: show the compiled strategy as an overlay on the canvas
- Collapsed groups highlighted with a shared boundary
- Parallel phases shown with ‖ indicators
- Convergent meshes shown with cycle arrows and iteration cap badge
- "Run as planned" or "Run sequential (safe mode)" choice

#### 5.9 Keyboard Shortcuts Overlay

- `?` key shows shortcuts cheat sheet modal
- Groups: Navigation, Editing, Execution, Debug
- Searchable with fuzzy matching

---

## Architectural Notes

### The Conductor + Foreman + Librarian Pipeline

When a flow executes under the full Conductor Protocol:

```
1. User clicks "Run" (or cron fires)
2. CONDUCTOR reads FlowGraph JSON
3.   → Identifies agent chains → marks for COLLAPSE
4.   → Identifies tool/http/code nodes → marks for EXTRACT
5.   → Identifies independent branches → marks for PARALLELIZE
6.   → Identifies cycles → marks for CONVERGE (mesh)
7.   → Produces ExecutionStrategy JSON
8. Validator checks strategy against graph (every node, dependencies)
9.   → If invalid: fall back to sequential executor
10. Executor walks strategy phases:
11.   → Collapsed agent units: 1 LLM call per unit
12.   → Extracted action units: LIBRARIAN discovers tools → FOREMAN executes via MCP
13.   → Parallel units: Promise.all
14.   → Mesh units: iterative rounds → convergence detection → synthesis
15. Results flow back; Conductor can re-compile mid-flight on failure
```

### n8n Bridge Integration

The Rust backend already has n8n API integration (`engine_n8n_list_workflows`,
`engine_n8n_trigger_workflow`, `engine_n8n_deploy_mcp_workflow`).

**Opportunity:** Add an "n8n Workflow" node kind that triggers an n8n workflow
and captures its output. The Conductor extracts these as direct-action units.
Instantly, 600+ n8n integrations available inside Pawz Flows at native speed,
with AI agents orchestrating them.

### Adaptive Conductor Threshold

The Conductor itself costs 1–3s (one LLM call to compile the strategy). For
trivial flows, this overhead exceeds the savings.

**Rule:** Invoke the Conductor only when the graph warrants it:
- 4+ nodes, OR
- Any branching (fan-out/fan-in), OR
- Any cycles (bidirectional edges), OR
- Mixed node types (agents + actions)

Below threshold: run the sequential executor directly. The Conductor is
additive, never mandatory.

### Test Coverage

Three test files cover atoms, executor-atoms, and parser (~620 lines).
Each new Conductor primitive needs its own test suite:
- `conductor.test.ts` — strategy compilation, collapse detection, extraction
  classification, parallel grouping, mesh detection
- `conductor-exec.test.ts` — strategy execution, phase ordering, fallback
- `convergent-mesh.test.ts` — iteration, convergence detection, forced synthesis

---

## Priority Matrix

| Phase | Impact | Effort | Dependencies |
|-------|--------|--------|--------------|
| Phase 0 (Quick Wins) | Noticeable | Small | None — do immediately |
| Phase 1 (Foundation) | High | Medium | None |
| Phase 2.1–2.2 (Extract + Parallelize) | **Critical** | Medium | None |
| Phase 2.3 (Collapse) | **Critical** | Medium | Phase 0.1 (event-driven) |
| Phase 2.4 (Converge) | High | Large | None |
| Phase 2.5 (Full Conductor) | **Transformative** | Large | Phases 2.1–2.4 |
| Phase 3 (Power) | High | Large | Phase 1 (persistence) |
| Phase 4 (AI) | Very High | Large | Phases 1 + 2 |
| Phase 5 (UI) | Medium | Medium | None — interleave |

**Recommended execution order:**

```
Week 1:  Phase 0 (quick wins) — free speed, ships today
Week 2:  Phase 2.1 (extract) + 2.2 (parallelize) — biggest speed jump
Week 3:  Phase 1.1 (persistence) + 1.2 (scheduling) — foundational
Week 4:  Phase 2.3 (collapse) — the key Conductor primitive
Week 5:  Phase 2.4 (converge) — the novel piece, needs iteration
Week 6:  Phase 2.5 (full Conductor agent) — ties it all together
Week 7+: Phase 5 (UI) interleaved with Phase 3 (power) and Phase 4 (AI)
```
